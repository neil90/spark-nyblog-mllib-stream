{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apache Spark Streaming Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Purpose of this is to use the ETL/Modeling/Streaming abilities of Apache Spark in one script. This is for showcasing the abilities of Spark and reference. This does not cover indepth model creation and feature reduction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Cassandra Setup\n",
    "Run python file to connect to Cassandra Cluster(Single Node for this example) and create Keyspace. Then create and load Train table and create table for Test data for inserting our csv streaming data into."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Establishing connection to default cluster 127.0.0.1:9024\n",
      "Dropping Keyspace sparkdb if exists.\n",
      "Creating sparkdb Keyspace\n",
      "Creating table ny_times_train in sparkdb Keyspace\n",
      "Create table to store stream test data\n",
      "Load CSV NYTimesBlogTrain and Insert into Table...\n",
      "\n",
      "Data successfully Inserted.\n"
     ]
    }
   ],
   "source": [
    "%run csv2cassandra.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Create SC and SqlContext\n",
    "Create SC and SqlContext and get Datastax Cassandra connector as PYSPARK_SUBMIT_ARGS "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SQLContext\n",
    "import os\n",
    "\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = \"\"\"--packages datastax:spark-cassandra-connector:1.6.0-s_2.10 pyspark-shell\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "conf = SparkConf()\\\n",
    "        .setMaster(\"local[2]\")\\\n",
    "        .setAppName('Spark Streaming Project')\\\n",
    "        .set('spark.cassandra.connection.host','127.0.0.1')\n",
    "        #.set(\"spark.rpc.netty.dispatcher.numThreads\",\"2\")\n",
    "sc = SparkContext(conf=conf)\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Cassandra NY training data\n",
    "Load the NY training csv file we stored into Cassandra into a Spark DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "nydata_raw = sqlContext.read\\\n",
    "    .format(\"org.apache.spark.sql.cassandra\")\\\n",
    "    .options(table=\"ny_times_train\", keyspace=\"sparkdb\")\\\n",
    "    .load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. UDF Feature Creation\n",
    "From here we import 2 functions from features file.\n",
    "\n",
    "registerFunctions -> Registers the functions to the SQLContext\n",
    "\n",
    "featurecreation -> Applies the features we just registered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from features import registerFunctions, featurecreation\n",
    "\n",
    "registerFunctions(sqlContext)\n",
    "nydata = featurecreation(nydata_raw, 'nydata_raw', sqlContext)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. TF-IDF \n",
    "Create TF-IDF features using the process_headline column it has punctuation/stop words removed and stemming applied to the original headline column. Also note we use the CountVectorizer instead of HashingTF. \n",
    "\n",
    "Differences between CountVectorizer and HashingTF -\n",
    "\n",
    "CountVectorizer allows us to call vocabulary function which keeps the list of words in the sparse matrix. So you can essentially see what word has what TF-IDF score.\n",
    "\n",
    "Where as HashingTF is one way once you apply it you can't go back. There is also is a possiblity of collision but this is rare.\n",
    "\n",
    "Depending on the size of your dataset it might be best to use HashingTF since there is extra memory required to store the vocabulary list.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Tokenizer, CountVectorizer, IDF\n",
    "\n",
    "tokenizer = Tokenizer(inputCol=\"process_headline\", outputCol=\"headline_token\")\n",
    "vectorizer = CountVectorizer(inputCol=\"headline_token\", outputCol=\"headline_vec\")\n",
    "idf = IDF(minDocFreq = 2, inputCol = 'headline_vec', outputCol ='tfidf_headline')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. StringIndexer and OneHotEncoding\n",
    "The 3 columns sectionname/subsectionname/newsdesk need to be converted to Binary values since we are using Logistic Regression which cannot take any categorical features. Store the indexers and encoders in a dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer\n",
    "def oHe(Ohecols):\n",
    "    ind = 'ind'\n",
    "    vec = 'vec'\n",
    "    \n",
    "    dictOhe = {}\n",
    "    \n",
    "    for col in Ohecols:\n",
    "        stringIndKey = col+ind\n",
    "        OheVecKey = col+vec\n",
    "        \n",
    "        stringIndexer = StringIndexer(inputCol=col, outputCol=stringIndKey)\n",
    "        encoder = OneHotEncoder(dropLast=False, inputCol = stringIndKey, outputCol=OheVecKey)\n",
    "        \n",
    "        dictOhe[stringIndKey] = stringIndexer\n",
    "        dictOhe[OheVecKey] = encoder\n",
    "        \n",
    "    return dictOhe\n",
    "\n",
    "OHE_Cols = ['sectionname', 'subsectionname', 'newsdesk']\n",
    "Ohe = oHe(OHE_Cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. VectorIndexer and VectorAssembler\n",
    "Combine all the Vector columns together using the Assembler so that we can easily apply the VectorIndexer to all the columns. The function getMaxCategories goes through all the Columns and gets a distinct count and returns the max. Apply the VectorIndexer function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorIndexer, VectorAssembler\n",
    "from pyspark.sql.functions import countDistinct\n",
    "\n",
    "vectorizeCols = ['question', 'religion', 'political', \n",
    "                 'socialmedia','dowpub', 'hourpub']\n",
    "                 #'monthpub'] \n",
    "                  #'monthpub', 'dowpub', 'hourpub']\n",
    "interm_assembler = VectorAssembler(inputCols=vectorizeCols, outputCol=\"raw_features\")\n",
    "\n",
    "def getMaxCategories(df, vecCols):\n",
    "    \n",
    "    numCategories = []\n",
    "    \n",
    "    for col in vecCols:\n",
    "        numCategories.append(df.agg(countDistinct(col)).collect()[0][0])\n",
    "        \n",
    "    return max(numCategories)\n",
    "\n",
    "maxCat = getMaxCategories(nydata, vectorizeCols)\n",
    "\n",
    "\n",
    "interm_indexer = VectorIndexer(inputCol=\"raw_features\", outputCol=\"features_numerical\", maxCategories=maxCat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Final Assembler\n",
    "Combine all the columns we are going to use in our model into one column called features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "\n",
    "features = [col for col in list(Ohe.keys()) if 'vec' in col]\\\n",
    "        + ['wordcount_norm', 'features_numerical', 'tfidf_headline']\n",
    "final_assembler = VectorAssembler(inputCols=features, outputCol=\"features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Logistic Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "lr = LogisticRegression(maxIter=10, regParam=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Train and Test Split\n",
    "Create train and test data set of nydata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nydata.cache()\n",
    "\n",
    "train, test = nydata.randomSplit([0.7, 0.3], seed=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Pipeline\n",
    "Create Pipeline to contain all the steps we are going to apply to the data. This means including One Hot Encoder/Assembler/Vectorizer/Logistic Regression Model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "\n",
    "pipeline = Pipeline(stages=[Ohe['newsdeskind'],Ohe['newsdeskvec'], \n",
    "                            Ohe['sectionnameind'], Ohe['sectionnamevec'],\n",
    "                            Ohe['subsectionnameind'], Ohe['subsectionnamevec'],\n",
    "                            interm_assembler, interm_indexer, tokenizer,\n",
    "                            vectorizer, idf, final_assembler, lr])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10a. Fit training data and transform test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/neil/spark-1.6.1-bin-hadoop2.6/python/pyspark/ml/classification.py:207: UserWarning: weights is deprecated. Use coefficients instead.\n",
      "  warnings.warn(\"weights is deprecated. Use coefficients instead.\")\n"
     ]
    }
   ],
   "source": [
    "model = pipeline.fit(train)\n",
    "\n",
    "prediction_test = model.transform(test) \n",
    "#This warning message has been removed in Spark 2.0 from Pull Request 12732"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Confusion Matrix and Model Accuracy\n",
    "Now lets collect the prediction and label and create a confusion matrix to determine the model accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predictionsAndLabels_test = prediction_test.map(lambda row: (row.prediction, row.label)).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def confusion_matrix(predAndLabel):\n",
    "    y_actual = pd.Series([x for x, y in predAndLabel], name = 'Actual')\n",
    "    y_pred = pd.Series([y for x, y in predAndLabel], name = 'Predicted')\n",
    "    \n",
    "    matrix = pd.crosstab(y_actual,y_pred)\n",
    "    accuracy = (matrix[0][0] + matrix[1][1])/ \\\n",
    "                (matrix[0][0] + matrix[0][1] + matrix[1][0] + matrix[1][1])\n",
    "\n",
    "    return matrix, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "Predicted   0.0  1.0\n",
      "Actual              \n",
      "0.0        1475  158\n",
      "1.0          96  192\n",
      "Logistic Model Accuracy: 0.8677771993753254\n"
     ]
    }
   ],
   "source": [
    "df_confusion_Logit, accuracy_Logit = confusion_matrix(predictionsAndLabels_test)\n",
    "\n",
    "print('Confusion Matrix:')\n",
    "print(df_confusion_Logit)\n",
    "print('Logistic Model Accuracy: {0}'.format(accuracy_Logit))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Spark Streaming\n",
    "To simulate streaming data first we will stream NYTimesBlogTest.csv to port 9999. Run the csv2stream.py file to start listening to the port."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.streaming import StreamingContext\n",
    "import sys\n",
    "\n",
    "ssc = StreamingContext(sc, 2)\n",
    "\n",
    "lines = ssc.socketTextStream(\"localhost\", 9999)\n",
    "\n",
    "lineRDD = lines.map(lambda line: (line.split(\"\\001\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11a. Function to transform stream and insert into Cassandra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "def streamrdd_to_df(rdd):\n",
    "\n",
    "    try:\n",
    "        #Create dataframe from raw text\n",
    "        sqlContext = SQLContext(rdd.context)        \n",
    "        rawCols = Row('newsdesk', 'sectionname', 'subsectionname',\n",
    "                  'headline', 'snippet', 'abstract', 'wordcount',\n",
    "                  'pubdate', 'uniqueid')\n",
    "        \n",
    "        raw_rdd = rdd.map(lambda r: rawCols(r[0], r[1], r[2], r[3],\n",
    "                                               r[4], r[5], r[6], r[7], r[8]))\n",
    "        \n",
    "        raw_df = sqlContext.createDataFrame(raw_rdd)\n",
    "        \n",
    "        #Create new DF with features\n",
    "        registerFunctions(sqlContext)\n",
    "        feature_df = featurecreation(raw_df, 'df', sqlContext)\n",
    "        \n",
    "        #Transform new Dataframe\n",
    "        prediction = model.transform(feature_df)\n",
    "        \n",
    "        #Recreate DF with original columns + probability\n",
    "        cassCols = Row('newsdesk', 'sectionname', 'subsectionname',\n",
    "          'headline', 'snippet', 'abstract', 'wordcount',\n",
    "          'pubdate', 'uniqueid', 'popular_probability')\n",
    "\n",
    "        pred_rdd = prediction.map(lambda r: cassCols(r.newsdesk, r.sectionname, \n",
    "                                                        r.subsectionname, r.headline,\n",
    "                                                        r.snippet, r.abstract,\n",
    "                                                        r.wordcount, r.pubdate,\n",
    "                                                        r.uniqueid, float(r.probability[0])))\n",
    "        \n",
    "        pred_df = sqlContext.createDataFrame(pred_rdd)\n",
    "\n",
    "        \n",
    "        #Insert the data into ny_times_test_stream table\n",
    "        sys.stdout.write('\\rInserting {0} rows...'.format(pred_df.count()))\n",
    "        \n",
    "        pred_df.write\\\n",
    "            .format(\"org.apache.spark.sql.cassandra\")\\\n",
    "            .options(table=\"ny_times_test_stream\", keyspace=\"sparkdb\")\\\n",
    "            .mode('append')\\\n",
    "            .save()\n",
    "\n",
    "    except ValueError:\n",
    "        print('RDD Empty')\n",
    "        \n",
    "    except Exception as e:\n",
    "        print (e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lineRDD.foreachRDD(streamrdd_to_df) #Passes each RDD into the function\n",
    "\n",
    "sys.stdout.flush() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11b. Start Stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inserting 20 rows..."
     ]
    }
   ],
   "source": [
    "ssc.start()             \n",
    "ssc.awaitTermination()  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
